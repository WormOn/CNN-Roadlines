Using TensorFlow backend.
X: (11487, 80, 160, 3) -> Y: (11487, 80, 160, 1)
2017-12-16 05:12:59.414958: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-16 05:12:59.415021: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-16 05:12:59.415027: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-16 05:12:59.415051: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, butthese are available on your machine and could speed up CPU computations.
2017-12-16 05:12:59.415056: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-16 05:12:59.512267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-12-16 05:12:59.512626: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties:
name: GeForce GTX 1050 Ti
major: 6 minor: 1 memoryClockRate (GHz) 1.62
pciBusID 0000:01:00.0
Total memory: 3.94GiB
Free memory: 3.20GiB
2017-12-16 05:12:59.512643: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0
2017-12-16 05:12:59.512667: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y
2017-12-16 05:12:59.512675: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0)
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to
====================================================================================================
input_1 (InputLayer)             (None, 80, 160, 3)    0
____________________________________________________________________________________________________
batch_normalization_1 (BatchNorm (None, 80, 160, 3)    12          input_1[0][0]
____________________________________________________________________________________________________
Conv0 (Conv2D)                   (None, 78, 158, 8)    224         batch_normalization_1[0][0]
____________________________________________________________________________________________________
Conv1C (Conv2D)                  (None, 76, 156, 16)   1168        Conv0[0][0]
____________________________________________________________________________________________________
Pooling1C (MaxPooling2D)         (None, 38, 78, 16)    0           Conv1C[0][0]
____________________________________________________________________________________________________
Conv2C (Conv2D)                  (None, 36, 76, 32)    4640        Pooling1C[0][0]
____________________________________________________________________________________________________
dropout_3 (Dropout)              (None, 36, 76, 32)    0           Conv2C[0][0]
____________________________________________________________________________________________________
Pooling2C (MaxPooling2D)         (None, 18, 38, 32)    0           dropout_3[0][0]
____________________________________________________________________________________________________
Conv3C (Conv2D)                  (None, 16, 36, 64)    18496       Pooling2C[0][0]
____________________________________________________________________________________________________
Conv1B (Conv2D)                  (None, 76, 156, 16)   1168        Conv0[0][0]
____________________________________________________________________________________________________
dropout_4 (Dropout)              (None, 16, 36, 64)    0           Conv3C[0][0]
____________________________________________________________________________________________________
Pooling1B (MaxPooling2D)         (None, 38, 78, 16)    0           Conv1B[0][0]
____________________________________________________________________________________________________
Pooling3C (MaxPooling2D)         (None, 8, 18, 64)     0           dropout_4[0][0]
____________________________________________________________________________________________________
Conv2B (Conv2D)                  (None, 36, 76, 32)    4640        Pooling1B[0][0]
____________________________________________________________________________________________________
UpSampling1C (UpSampling2D)      (None, 16, 36, 64)    0           Pooling3C[0][0]
____________________________________________________________________________________________________
dropout_2 (Dropout)              (None, 36, 76, 32)    0           Conv2B[0][0]
____________________________________________________________________________________________________
Deconv1C (Conv2DTranspose)       (None, 18, 38, 64)    36928       UpSampling1C[0][0]
____________________________________________________________________________________________________
Conv1A (Conv2D)                  (None, 76, 156, 16)   1168        Conv0[0][0]
____________________________________________________________________________________________________
Pooling2B (MaxPooling2D)         (None, 18, 38, 32)    0           dropout_2[0][0]
____________________________________________________________________________________________________
dropout_5 (Dropout)              (None, 18, 38, 64)    0           Deconv1C[0][0]
____________________________________________________________________________________________________
dropout_1 (Dropout)              (None, 76, 156, 16)   0           Conv1A[0][0]
____________________________________________________________________________________________________
UpSampling1B (UpSampling2D)      (None, 36, 76, 32)    0           Pooling2B[0][0]
____________________________________________________________________________________________________
UpSampling2C (UpSampling2D)      (None, 36, 76, 64)    0           dropout_5[0][0]
____________________________________________________________________________________________________
Pooling1A (MaxPooling2D)         (None, 38, 78, 16)    0           dropout_1[0][0]
____________________________________________________________________________________________________
Deconv1B (Conv2DTranspose)       (None, 38, 78, 32)    9248        UpSampling1B[0][0]
____________________________________________________________________________________________________
Deconv2C (Conv2DTranspose)       (None, 38, 78, 32)    18464       UpSampling2C[0][0]
____________________________________________________________________________________________________
UpSampling1A (UpSampling2D)      (None, 76, 156, 16)   0           Pooling1A[0][0]
____________________________________________________________________________________________________
dropout_7 (Dropout)              (None, 38, 78, 32)    0           Deconv1B[0][0]
____________________________________________________________________________________________________
dropout_6 (Dropout)              (None, 38, 78, 32)    0           Deconv2C[0][0]
____________________________________________________________________________________________________
Deconv1A (Conv2DTranspose)       (None, 78, 158, 16)   2320        UpSampling1A[0][0]
____________________________________________________________________________________________________
UpSampling2B (UpSampling2D)      (None, 76, 156, 32)   0           dropout_7[0][0]
____________________________________________________________________________________________________
UpSampling3C (UpSampling2D)      (None, 76, 156, 32)   0           dropout_6[0][0]
____________________________________________________________________________________________________
dropout_8 (Dropout)              (None, 78, 158, 16)   0           Deconv1A[0][0]
____________________________________________________________________________________________________
Deconv2B (Conv2DTranspose)       (None, 78, 158, 16)   4624        UpSampling2B[0][0]
____________________________________________________________________________________________________
Deconv3C (Conv2DTranspose)       (None, 78, 158, 16)   4624        UpSampling3C[0][0]
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 78, 158, 48)   0           dropout_8[0][0]
                                                                   Deconv2B[0][0]
                                                                   Deconv3C[0][0]
____________________________________________________________________________________________________
Deconv0 (Conv2DTranspose)        (None, 80, 160, 1)    433         concatenate_1[0][0]
====================================================================================================
Total params: 108,157
Trainable params: 108,151
Non-trainable params: 6
____________________________________________________________________________________________________
Epoch 1/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9278Epoch 00000: saving model to weights_dropout/weights.00-0.0384-0.9278.hdf5
11487/11487 [==============================] - 93s - loss: 0.0384 - acc: 0.9278
Epoch 2/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9566Epoch 00001: saving model to weights_dropout/weights.01-0.0100-0.9566.hdf5
11487/11487 [==============================] - 86s - loss: 0.0100 - acc: 0.9566
Epoch 3/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9586Epoch 00002: saving model to weights_dropout/weights.02-0.0077-0.9586.hdf5
11487/11487 [==============================] - 84s - loss: 0.0077 - acc: 0.9586
Epoch 4/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9594Epoch 00003: saving model to weights_dropout/weights.03-0.0068-0.9594.hdf5
11487/11487 [==============================] - 83s - loss: 0.0068 - acc: 0.9594
Epoch 5/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9598Epoch 00004: saving model to weights_dropout/weights.04-0.0063-0.9598.hdf5
11487/11487 [==============================] - 83s - loss: 0.0063 - acc: 0.9598
Epoch 6/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0059 - acc: 0.9601Epoch 00005: saving model to weights_dropout/weights.05-0.0059-0.9601.hdf5
11487/11487 [==============================] - 83s - loss: 0.0059 - acc: 0.9601
Epoch 7/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0057 - acc: 0.9603Epoch 00006: saving model to weights_dropout/weights.06-0.0057-0.9603.hdf5
11487/11487 [==============================] - 84s - loss: 0.0057 - acc: 0.9603
Epoch 8/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9605Epoch 00007: saving model to weights_dropout/weights.07-0.0054-0.9605.hdf5
11487/11487 [==============================] - 83s - loss: 0.0054 - acc: 0.9605
Epoch 9/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9606Epoch 00008: saving model to weights_dropout/weights.08-0.0053-0.9606.hdf5
11487/11487 [==============================] - 83s - loss: 0.0053 - acc: 0.9606
Epoch 10/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0052 - acc: 0.9606Epoch 00009: saving model to weights_dropout/weights.09-0.0052-0.9606.hdf5
11487/11487 [==============================] - 83s - loss: 0.0052 - acc: 0.9606
Epoch 11/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9609Epoch 00010: saving model to weights_dropout/weights.10-0.0050-0.9609.hdf5
11487/11487 [==============================] - 83s - loss: 0.0050 - acc: 0.9609
Epoch 12/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9609Epoch 00011: saving model to weights_dropout/weights.11-0.0049-0.9609.hdf5
11487/11487 [==============================] - 83s - loss: 0.0049 - acc: 0.9609
Epoch 13/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9610Epoch 00012: saving model to weights_dropout/weights.12-0.0048-0.9610.hdf5
11487/11487 [==============================] - 83s - loss: 0.0048 - acc: 0.9610
Epoch 14/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9610Epoch 00013: saving model to weights_dropout/weights.13-0.0048-0.9610.hdf5
11487/11487 [==============================] - 83s - loss: 0.0048 - acc: 0.9610
Epoch 15/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9611Epoch 00014: saving model to weights_dropout/weights.14-0.0047-0.9611.hdf5
11487/11487 [==============================] - 83s - loss: 0.0047 - acc: 0.9611
Epoch 16/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9612Epoch 00015: saving model to weights_dropout/weights.15-0.0045-0.9612.hdf5
11487/11487 [==============================] - 83s - loss: 0.0045 - acc: 0.9612
Epoch 17/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9613Epoch 00016: saving model to weights_dropout/weights.16-0.0045-0.9613.hdf5
11487/11487 [==============================] - 83s - loss: 0.0045 - acc: 0.9613
Epoch 18/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9612Epoch 00017: saving model to weights_dropout/weights.17-0.0045-0.9612.hdf5
11487/11487 [==============================] - 83s - loss: 0.0045 - acc: 0.9612
Epoch 19/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9614Epoch 00018: saving model to weights_dropout/weights.18-0.0043-0.9614.hdf5
11487/11487 [==============================] - 83s - loss: 0.0043 - acc: 0.9614
Epoch 20/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9614Epoch 00019: saving model to weights_dropout/weights.19-0.0043-0.9614.hdf5
11487/11487 [==============================] - 83s - loss: 0.0043 - acc: 0.9614
Epoch 21/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9615Epoch 00020: saving model to weights_dropout/weights.20-0.0042-0.9615.hdf5
11487/11487 [==============================] - 82s - loss: 0.0042 - acc: 0.9615
Epoch 22/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9615Epoch 00021: saving model to weights_dropout/weights.21-0.0043-0.9615.hdf5
11487/11487 [==============================] - 82s - loss: 0.0043 - acc: 0.9615
Epoch 23/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9615Epoch 00022: saving model to weights_dropout/weights.22-0.0042-0.9615.hdf5
11487/11487 [==============================] - 82s - loss: 0.0042 - acc: 0.9615
Epoch 24/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0041 - acc: 0.9616Epoch 00023: saving model to weights_dropout/weights.23-0.0041-0.9616.hdf5
11487/11487 [==============================] - 82s - loss: 0.0041 - acc: 0.9616
Epoch 25/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0041 - acc: 0.9617Epoch 00024: saving model to weights_dropout/weights.24-0.0041-0.9617.hdf5
11487/11487 [==============================] - 82s - loss: 0.0041 - acc: 0.9617
Epoch 26/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9617Epoch 00025: saving model to weights_dropout/weights.25-0.0040-0.9617.hdf5
11487/11487 [==============================] - 82s - loss: 0.0040 - acc: 0.9617
Epoch 27/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9617Epoch 00026: saving model to weights_dropout/weights.26-0.0040-0.9617.hdf5
11487/11487 [==============================] - 82s - loss: 0.0040 - acc: 0.9617
Epoch 28/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9617Epoch 00027: saving model to weights_dropout/weights.27-0.0040-0.9617.hdf5
11487/11487 [==============================] - 82s - loss: 0.0040 - acc: 0.9617
Epoch 29/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9618Epoch 00028: saving model to weights_dropout/weights.28-0.0039-0.9618.hdf5
11487/11487 [==============================] - 82s - loss: 0.0039 - acc: 0.9618
Epoch 30/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9618Epoch 00029: saving model to weights_dropout/weights.29-0.0039-0.9618.hdf5
11487/11487 [==============================] - 82s - loss: 0.0039 - acc: 0.9618
Epoch 31/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9618Epoch 00030: saving model to weights_dropout/weights.30-0.0039-0.9618.hdf5
11487/11487 [==============================] - 82s - loss: 0.0039 - acc: 0.9618
Epoch 32/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9619Epoch 00031: saving model to weights_dropout/weights.31-0.0038-0.9619.hdf5
11487/11487 [==============================] - 82s - loss: 0.0038 - acc: 0.9619
Epoch 33/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9619Epoch 00032: saving model to weights_dropout/weights.32-0.0038-0.9619.hdf5
11487/11487 [==============================] - 82s - loss: 0.0038 - acc: 0.9619
Epoch 34/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9619Epoch 00033: saving model to weights_dropout/weights.33-0.0038-0.9619.hdf5
11487/11487 [==============================] - 82s - loss: 0.0038 - acc: 0.9619
Epoch 35/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9619Epoch 00034: saving model to weights_dropout/weights.34-0.0037-0.9619.hdf5
11487/11487 [==============================] - 82s - loss: 0.0037 - acc: 0.9619
Epoch 36/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9620Epoch 00035: saving model to weights_dropout/weights.35-0.0037-0.9620.hdf5
11487/11487 [==============================] - 82s - loss: 0.0037 - acc: 0.9620
Epoch 37/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9621Epoch 00036: saving model to weights_dropout/weights.36-0.0036-0.9621.hdf5
11487/11487 [==============================] - 82s - loss: 0.0036 - acc: 0.9621
Epoch 38/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9620Epoch 00037: saving model to weights_dropout/weights.37-0.0037-0.9620.hdf5
11487/11487 [==============================] - 82s - loss: 0.0037 - acc: 0.9620
Epoch 39/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9620Epoch 00038: saving model to weights_dropout/weights.38-0.0037-0.9620.hdf5
11487/11487 [==============================] - 82s - loss: 0.0037 - acc: 0.9620
Epoch 40/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9620Epoch 00039: saving model to weights_dropout/weights.39-0.0036-0.9620.hdf5
11487/11487 [==============================] - 82s - loss: 0.0036 - acc: 0.9620
Epoch 41/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9621Epoch 00040: saving model to weights_dropout/weights.40-0.0036-0.9621.hdf5
11487/11487 [==============================] - 82s - loss: 0.0036 - acc: 0.9621
Epoch 42/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9621Epoch 00041: saving model to weights_dropout/weights.41-0.0035-0.9621.hdf5
11487/11487 [==============================] - 82s - loss: 0.0035 - acc: 0.9621
Epoch 43/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9621Epoch 00042: saving model to weights_dropout/weights.42-0.0036-0.9620.hdf5
11487/11487 [==============================] - 82s - loss: 0.0036 - acc: 0.9620
Epoch 44/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9621Epoch 00043: saving model to weights_dropout/weights.43-0.0036-0.9621.hdf5
11487/11487 [==============================] - 82s - loss: 0.0036 - acc: 0.9621
Epoch 45/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9621Epoch 00044: saving model to weights_dropout/weights.44-0.0035-0.9621.hdf5
11487/11487 [==============================] - 82s - loss: 0.0035 - acc: 0.9621
Epoch 46/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9622Epoch 00045: saving model to weights_dropout/weights.45-0.0035-0.9622.hdf5
11487/11487 [==============================] - 82s - loss: 0.0035 - acc: 0.9622
Epoch 47/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9622Epoch 00046: saving model to weights_dropout/weights.46-0.0035-0.9622.hdf5
11487/11487 [==============================] - 82s - loss: 0.0035 - acc: 0.9622
Epoch 48/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9622Epoch 00047: saving model to weights_dropout/weights.47-0.0034-0.9622.hdf5
11487/11487 [==============================] - 82s - loss: 0.0034 - acc: 0.9622
Epoch 49/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9622Epoch 00048: saving model to weights_dropout/weights.48-0.0035-0.9622.hdf5
11487/11487 [==============================] - 82s - loss: 0.0035 - acc: 0.9622
Epoch 50/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9622Epoch 00049: saving model to weights_dropout/weights.49-0.0035-0.9622.hdf5
11487/11487 [==============================] - 82s - loss: 0.0035 - acc: 0.9622
Epoch 51/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9623Epoch 00050: saving model to weights_dropout/weights.50-0.0034-0.9623.hdf5
11487/11487 [==============================] - 82s - loss: 0.0034 - acc: 0.9623
Epoch 52/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9623Epoch 00051: saving model to weights_dropout/weights.51-0.0034-0.9623.hdf5
11487/11487 [==============================] - 82s - loss: 0.0034 - acc: 0.9623
Epoch 53/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9623Epoch 00052: saving model to weights_dropout/weights.52-0.0034-0.9623.hdf5
11487/11487 [==============================] - 82s - loss: 0.0034 - acc: 0.9623
Epoch 54/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9623Epoch 00053: saving model to weights_dropout/weights.53-0.0034-0.9623.hdf5
11487/11487 [==============================] - 82s - loss: 0.0034 - acc: 0.9623
Epoch 55/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9623Epoch 00054: saving model to weights_dropout/weights.54-0.0033-0.9623.hdf5
11487/11487 [==============================] - 82s - loss: 0.0033 - acc: 0.9623
Epoch 56/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9623Epoch 00055: saving model to weights_dropout/weights.55-0.0034-0.9623.hdf5
11487/11487 [==============================] - 82s - loss: 0.0034 - acc: 0.9623
Epoch 57/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9623Epoch 00056: saving model to weights_dropout/weights.56-0.0033-0.9623.hdf5
11487/11487 [==============================] - 82s - loss: 0.0033 - acc: 0.9623
Epoch 58/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9623Epoch 00057: saving model to weights_dropout/weights.57-0.0033-0.9623.hdf5
11487/11487 [==============================] - 82s - loss: 0.0033 - acc: 0.9623
Epoch 59/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9624Epoch 00058: saving model to weights_dropout/weights.58-0.0033-0.9624.hdf5
11487/11487 [==============================] - 82s - loss: 0.0033 - acc: 0.9624
Epoch 60/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9623Epoch 00059: saving model to weights_dropout/weights.59-0.0033-0.9623.hdf5
11487/11487 [==============================] - 82s - loss: 0.0033 - acc: 0.9623
Epoch 61/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9624Epoch 00060: saving model to weights_dropout/weights.60-0.0033-0.9624.hdf5
11487/11487 [==============================] - 82s - loss: 0.0033 - acc: 0.9624
Epoch 62/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9624Epoch 00061: saving model to weights_dropout/weights.61-0.0033-0.9624.hdf5
11487/11487 [==============================] - 82s - loss: 0.0033 - acc: 0.9624
Epoch 63/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9624Epoch 00062: saving model to weights_dropout/weights.62-0.0033-0.9624.hdf5
11487/11487 [==============================] - 82s - loss: 0.0033 - acc: 0.9624
Epoch 64/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9624Epoch 00063: saving model to weights_dropout/weights.63-0.0033-0.9624.hdf5
11487/11487 [==============================] - 82s - loss: 0.0033 - acc: 0.9624
Epoch 65/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9624Epoch 00064: saving model to weights_dropout/weights.64-0.0033-0.9624.hdf5
11487/11487 [==============================] - 82s - loss: 0.0033 - acc: 0.9624
Epoch 66/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9624Epoch 00065: saving model to weights_dropout/weights.65-0.0032-0.9624.hdf5
11487/11487 [==============================] - 82s - loss: 0.0032 - acc: 0.9624
Epoch 67/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9624Epoch 00066: saving model to weights_dropout/weights.66-0.0032-0.9624.hdf5
11487/11487 [==============================] - 82s - loss: 0.0032 - acc: 0.9624
Epoch 68/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9624Epoch 00067: saving model to weights_dropout/weights.67-0.0032-0.9624.hdf5
11487/11487 [==============================] - 82s - loss: 0.0032 - acc: 0.9624
Epoch 69/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9624Epoch 00068: saving model to weights_dropout/weights.68-0.0032-0.9624.hdf5
11487/11487 [==============================] - 82s - loss: 0.0032 - acc: 0.9624
Epoch 70/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9624Epoch 00069: saving model to weights_dropout/weights.69-0.0032-0.9624.hdf5
11487/11487 [==============================] - 82s - loss: 0.0032 - acc: 0.9624
Epoch 71/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9624Epoch 00070: saving model to weights_dropout/weights.70-0.0032-0.9624.hdf5
11487/11487 [==============================] - 82s - loss: 0.0032 - acc: 0.9624
Epoch 72/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9625Epoch 00071: saving model to weights_dropout/weights.71-0.0032-0.9625.hdf5
11487/11487 [==============================] - 82s - loss: 0.0032 - acc: 0.9625
Epoch 73/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9625Epoch 00072: saving model to weights_dropout/weights.72-0.0031-0.9625.hdf5
11487/11487 [==============================] - 82s - loss: 0.0031 - acc: 0.9625
Epoch 74/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9623Epoch 00073: saving model to weights_dropout/weights.73-0.0033-0.9623.hdf5
11487/11487 [==============================] - 82s - loss: 0.0033 - acc: 0.9623
Epoch 75/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9625Epoch 00074: saving model to weights_dropout/weights.74-0.0032-0.9625.hdf5
11487/11487 [==============================] - 82s - loss: 0.0032 - acc: 0.9625
Epoch 76/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9625Epoch 00075: saving model to weights_dropout/weights.75-0.0031-0.9625.hdf5
11487/11487 [==============================] - 82s - loss: 0.0031 - acc: 0.9625
Epoch 77/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9625Epoch 00076: saving model to weights_dropout/weights.76-0.0031-0.9625.hdf5
11487/11487 [==============================] - 82s - loss: 0.0031 - acc: 0.9625
Epoch 78/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9625Epoch 00077: saving model to weights_dropout/weights.77-0.0031-0.9625.hdf5
11487/11487 [==============================] - 82s - loss: 0.0031 - acc: 0.9625
Epoch 79/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9625Epoch 00078: saving model to weights_dropout/weights.78-0.0031-0.9625.hdf5
11487/11487 [==============================] - 82s - loss: 0.0031 - acc: 0.9625
Epoch 80/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9626Epoch 00079: saving model to weights_dropout/weights.79-0.0031-0.9626.hdf5
11487/11487 [==============================] - 82s - loss: 0.0031 - acc: 0.9626
Epoch 81/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9625Epoch 00080: saving model to weights_dropout/weights.80-0.0031-0.9625.hdf5
11487/11487 [==============================] - 82s - loss: 0.0031 - acc: 0.9625
Epoch 82/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9625Epoch 00081: saving model to weights_dropout/weights.81-0.0031-0.9625.hdf5
11487/11487 [==============================] - 82s - loss: 0.0031 - acc: 0.9625
Epoch 83/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9625Epoch 00082: saving model to weights_dropout/weights.82-0.0031-0.9625.hdf5
11487/11487 [==============================] - 82s - loss: 0.0031 - acc: 0.9625
Epoch 84/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9625Epoch 00083: saving model to weights_dropout/weights.83-0.0031-0.9625.hdf5
11487/11487 [==============================] - 82s - loss: 0.0031 - acc: 0.9625
Epoch 85/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9626Epoch 00084: saving model to weights_dropout/weights.84-0.0031-0.9626.hdf5
11487/11487 [==============================] - 82s - loss: 0.0031 - acc: 0.9626
Epoch 86/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9626Epoch 00085: saving model to weights_dropout/weights.85-0.0031-0.9626.hdf5
11487/11487 [==============================] - 82s - loss: 0.0031 - acc: 0.9626
Epoch 87/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9625Epoch 00086: saving model to weights_dropout/weights.86-0.0031-0.9625.hdf5
11487/11487 [==============================] - 82s - loss: 0.0031 - acc: 0.9625
Epoch 88/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9625Epoch 00087: saving model to weights_dropout/weights.87-0.0031-0.9625.hdf5
11487/11487 [==============================] - 82s - loss: 0.0031 - acc: 0.9625
Epoch 89/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9626Epoch 00088: saving model to weights_dropout/weights.88-0.0030-0.9626.hdf5
11487/11487 [==============================] - 82s - loss: 0.0030 - acc: 0.9626
Epoch 90/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9626Epoch 00089: saving model to weights_dropout/weights.89-0.0030-0.9626.hdf5
11487/11487 [==============================] - 82s - loss: 0.0030 - acc: 0.9626
Epoch 91/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9626Epoch 00090: saving model to weights_dropout/weights.90-0.0031-0.9626.hdf5
11487/11487 [==============================] - 82s - loss: 0.0031 - acc: 0.9626
Epoch 92/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9626Epoch 00091: saving model to weights_dropout/weights.91-0.0030-0.9626.hdf5
11487/11487 [==============================] - 82s - loss: 0.0030 - acc: 0.9626
Epoch 93/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9625Epoch 00092: saving model to weights_dropout/weights.92-0.0031-0.9625.hdf5
11487/11487 [==============================] - 82s - loss: 0.0031 - acc: 0.9625
Epoch 94/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9626Epoch 00093: saving model to weights_dropout/weights.93-0.0030-0.9626.hdf5
11487/11487 [==============================] - 82s - loss: 0.0030 - acc: 0.9626
Epoch 95/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9626Epoch 00094: saving model to weights_dropout/weights.94-0.0031-0.9626.hdf5
11487/11487 [==============================] - 82s - loss: 0.0031 - acc: 0.9626
Epoch 96/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9627Epoch 00095: saving model to weights_dropout/weights.95-0.0030-0.9627.hdf5
11487/11487 [==============================] - 82s - loss: 0.0030 - acc: 0.9627
Epoch 97/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9626Epoch 00096: saving model to weights_dropout/weights.96-0.0030-0.9626.hdf5
11487/11487 [==============================] - 82s - loss: 0.0030 - acc: 0.9626
Epoch 98/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9626Epoch 00097: saving model to weights_dropout/weights.97-0.0030-0.9626.hdf5
11487/11487 [==============================] - 82s - loss: 0.0030 - acc: 0.9626
Epoch 99/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9627Epoch 00098: saving model to weights_dropout/weights.98-0.0030-0.9627.hdf5
11487/11487 [==============================] - 82s - loss: 0.0030 - acc: 0.9627
Epoch 100/100
11480/11487 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9627Epoch 00099: saving model to weights_dropout/weights.99-0.0030-0.9627.hdf5
11487/11487 [==============================] - 82s - loss: 0.0030 - acc: 0.9627
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to
====================================================================================================
input_1 (InputLayer)             (None, 80, 160, 3)    0
____________________________________________________________________________________________________
batch_normalization_1 (BatchNorm (None, 80, 160, 3)    12          input_1[0][0]
____________________________________________________________________________________________________
Conv0 (Conv2D)                   (None, 78, 158, 8)    224         batch_normalization_1[0][0]
____________________________________________________________________________________________________
Conv1C (Conv2D)                  (None, 76, 156, 16)   1168        Conv0[0][0]
____________________________________________________________________________________________________
Pooling1C (MaxPooling2D)         (None, 38, 78, 16)    0           Conv1C[0][0]
____________________________________________________________________________________________________
Conv2C (Conv2D)                  (None, 36, 76, 32)    4640        Pooling1C[0][0]
____________________________________________________________________________________________________
dropout_3 (Dropout)              (None, 36, 76, 32)    0           Conv2C[0][0]
____________________________________________________________________________________________________
Pooling2C (MaxPooling2D)         (None, 18, 38, 32)    0           dropout_3[0][0]
____________________________________________________________________________________________________
Conv3C (Conv2D)                  (None, 16, 36, 64)    18496       Pooling2C[0][0]
____________________________________________________________________________________________________
Conv1B (Conv2D)                  (None, 76, 156, 16)   1168        Conv0[0][0]
____________________________________________________________________________________________________
dropout_4 (Dropout)              (None, 16, 36, 64)    0           Conv3C[0][0]
____________________________________________________________________________________________________
Pooling1B (MaxPooling2D)         (None, 38, 78, 16)    0           Conv1B[0][0]
____________________________________________________________________________________________________
Pooling3C (MaxPooling2D)         (None, 8, 18, 64)     0           dropout_4[0][0]
____________________________________________________________________________________________________
Conv2B (Conv2D)                  (None, 36, 76, 32)    4640        Pooling1B[0][0]
____________________________________________________________________________________________________
UpSampling1C (UpSampling2D)      (None, 16, 36, 64)    0           Pooling3C[0][0]
____________________________________________________________________________________________________
dropout_2 (Dropout)              (None, 36, 76, 32)    0           Conv2B[0][0]
____________________________________________________________________________________________________
Deconv1C (Conv2DTranspose)       (None, 18, 38, 64)    36928       UpSampling1C[0][0]
____________________________________________________________________________________________________
Conv1A (Conv2D)                  (None, 76, 156, 16)   1168        Conv0[0][0]
____________________________________________________________________________________________________
Pooling2B (MaxPooling2D)         (None, 18, 38, 32)    0           dropout_2[0][0]
____________________________________________________________________________________________________
dropout_5 (Dropout)              (None, 18, 38, 64)    0           Deconv1C[0][0]
____________________________________________________________________________________________________
dropout_1 (Dropout)              (None, 76, 156, 16)   0           Conv1A[0][0]
____________________________________________________________________________________________________
UpSampling1B (UpSampling2D)      (None, 36, 76, 32)    0           Pooling2B[0][0]
____________________________________________________________________________________________________
UpSampling2C (UpSampling2D)      (None, 36, 76, 64)    0           dropout_5[0][0]
____________________________________________________________________________________________________
Pooling1A (MaxPooling2D)         (None, 38, 78, 16)    0           dropout_1[0][0]
____________________________________________________________________________________________________
Deconv1B (Conv2DTranspose)       (None, 38, 78, 32)    9248        UpSampling1B[0][0]
____________________________________________________________________________________________________
Deconv2C (Conv2DTranspose)       (None, 38, 78, 32)    18464       UpSampling2C[0][0]
____________________________________________________________________________________________________
UpSampling1A (UpSampling2D)      (None, 76, 156, 16)   0           Pooling1A[0][0]
____________________________________________________________________________________________________
dropout_7 (Dropout)              (None, 38, 78, 32)    0           Deconv1B[0][0]
____________________________________________________________________________________________________
dropout_6 (Dropout)              (None, 38, 78, 32)    0           Deconv2C[0][0]
____________________________________________________________________________________________________
Deconv1A (Conv2DTranspose)       (None, 78, 158, 16)   2320        UpSampling1A[0][0]
____________________________________________________________________________________________________
UpSampling2B (UpSampling2D)      (None, 76, 156, 32)   0           dropout_7[0][0]
____________________________________________________________________________________________________
UpSampling3C (UpSampling2D)      (None, 76, 156, 32)   0           dropout_6[0][0]
____________________________________________________________________________________________________
dropout_8 (Dropout)              (None, 78, 158, 16)   0           Deconv1A[0][0]
____________________________________________________________________________________________________
Deconv2B (Conv2DTranspose)       (None, 78, 158, 16)   4624        UpSampling2B[0][0]
____________________________________________________________________________________________________
Deconv3C (Conv2DTranspose)       (None, 78, 158, 16)   4624        UpSampling3C[0][0]
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 78, 158, 48)   0           dropout_8[0][0]
                                                                   Deconv2B[0][0]
                                                                   Deconv3C[0][0]
____________________________________________________________________________________________________
Deconv0 (Conv2DTranspose)        (None, 80, 160, 1)    433         concatenate_1[0][0]
====================================================================================================
Total params: 108,157
Trainable params: 108,151
Non-trainable params: 6
____________________________________________________________________________________________________